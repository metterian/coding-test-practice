# To add a new cell, type '# %%'
# To add a new markdown cell, type '# %% [markdown]'
# %%
from IPython import get_ipython

# %% [markdown]
# # 2021 NLP & AI Coding Test
#
# ![](https://item.kakaocdn.net/do/8ad594ffa619c5063075206d55ec79a18f324a0b9c48f77dbce3a43bd11ce785)
#
# - ì•ˆë…•í•˜ì„¸ìš”! 2021 NLP & AI ì—°êµ¬ì‹¤ì˜ ì½”ë”© í…ŒìŠ¤íŠ¸ì— ì˜¤ì‹  ì—¬ëŸ¬ë¶„ í™˜ì˜í•©ë‹ˆë‹¤. ğŸ‘¨
# - ë³¸ í…ŒìŠ¤íŠ¸ëŠ” íŒŒì´ì¬(python)ğŸ ì–¸ì–´ë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ì–‘í•œ í”„ë¡œê·¸ë˜ë° ìƒí™©ì—ì„œì˜ ê°„ë‹¨í•œ í€´ì¦ˆë¥¼ í†µí•´ ì—¬ëŸ¬ë¶„ë“¤ì—ê²Œ í•„ìš”í•œ í•µì‹¬ ì—­ëŸ‰ì„ íŒŒì•…í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•©ë‹ˆë‹¤.
# - ì—¬ëŸ¬ë¶„ë“¤ì€ ì´ë¯¸ ì¸í„´ìœ¼ë¡œ NLP & AI ì—°êµ¬ì‹¤ì˜ ì¸ì›ìœ¼ë¡œ í•©ë¥˜í•˜ì…¨ìœ¼ë©° ğŸ‘, ê·¸ ëŠ¥ë ¥ì˜ ì¶œì¤‘í•¨ì€ ì´ë¯¸ í‰ê°€ë˜ì—ˆë‹¤ ìƒê°í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ë¬´ë¦¬í•˜ê²Œ ì •ë‹µì„ ë§ì¶”ëŠ” ê²ƒì€ ë³¸ í…ŒìŠ¤íŠ¸ì˜ ëª©ì ì€ ì•„ë‹ˆë¯€ë¡œ ë¶€ë‹´ ì—†ì´ ë‡Œ ìš´ë™í•œë‹¤ ìƒê°í•˜ê³  ì°¸ì—¬í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤. ğŸï¸

# %%
# ì‹¤í–‰í•´ì£¼ì„¸ìš”!
get_ipython().system('git clone https://github.com/taeminlee/cote_vol_1')

# %% [markdown]
# # ì„¸ì…˜ 1 : íŒŒì´ì¬ ì•„ì¼ëœë“œ
#
# ![](https://images-na.ssl-images-amazon.com/images/S/pv-target-images/7548d4378a9ebcc60173995446a08014b114efc66e4713a4f292205981a5cf62._RI_V_TTW_.jpg)
# %% [markdown]
# ### Q1 : KorQuAD ë°ì´í„° ì§‘í•©ì—ì„œ `question` í‚¤ì˜ ê°’ë§Œ ê°€ì§€ëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ê³ ì í•œë‹¤. ì´ë¥¼ í•œì¤„ì˜ íŒŒì´ì¬ ì½”ë“œë¡œ ì‘ì„±í•˜ì‹œì˜¤.
#
# - KorQuAD ë°ì´í„° ì§‘í•©ì˜ ìŠ¤í‚¤ë§ˆëŠ” ë‹¤ìŒê³¼ ê°™ìŒ
#
# ```
# {
#   "version": string
#   "data" : [
#     "paragraphs" : [
#       "qas" : [
#         "question" : string,
#         "id" : string,
#         "answers" : [
#           "text" : string,
#           "answer_start" integer
#         ]
#     ]
#   ]
# }
# ```

# %%
from cote_vol_1.q1 import Q1
import pprint

q1_dataset = Q1.load_dataset()


# %%
# answer = [list(map(lambda y: y['question'], qa)) for qa in map(lambda x: x['qas'], data['paragraphs']) for data in q1_dataset['data']]
answer = []
for data in q1_dataset['data']:
    for qa in map(lambda x: x['qas'], data['paragraphs']):
        answer.append(list(map(lambda y: y['question'], qa)))


# %%
from itertools import chain
answer = list(chain.from_iterable([list(chain(map(lambda t: t['question'], k))) for k in (map(lambda y: y['qas'] ,paragraph)) for paragraph in map(lambda x: x['paragraphs'] ,q1_dataset['data'])]))

len(answer)


# %%
# questions = # ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”

questions = list(chain.from_iterable(answer))

get_ipython().run_line_magic('time', 'Q1.validate(questions)')

# %% [markdown]
# ### Q2 : $n$ ì‚¬ì´ì¦ˆì˜ ë¦¬ìŠ¤íŠ¸ì˜ ê°ê°ì˜ ì›ì†Œ $e$ë¥¼ ê²€ì‚¬í•˜ì—¬ ì§ìˆ˜ì¸ ê²½ìš° $e^2$ ê°’ìœ¼ë¡œ, í™€ìˆ˜ì¸ ê²½ìš° $\sqrt{e}$ë¡œ ë³€í™˜í•˜ëŠ” ì´í„°ë ˆì´í„°(iterator)ë¥¼ ì‘ì„±í•˜ì‹œì˜¤.

# %%
from cote_vol_1.q2 import Q2
import math

q2_dataset = Q2.load_dataset()
print("ë°ì´í„° ì§‘í•©ì˜ í¬ê¸° :", len(q2_dataset))


# %%
class Iterator:
    def __init__(self, dataset) -> None:
        self.dataset = dataset
        self.length = len(dataset)
        self.current = 0

    def __iter__(self):
        return self

    def __next__(self):
        if self.current > self.length:
            raise StopIteration
        if self.dataset[self.current] % 2 == 0:
            num = math.pow(self.dataset[self.current] , 2)
            self.current += 1
            return num
        else:
            num = math.sqrt(float(self.dataset[self.current] ))
            self.current += 1
            return num


# %%
iterator = Iterator(q2_dataset)
get_ipython().run_line_magic('time', 'Q2.validate(iterator)')

# %% [markdown]
# ### Q3 : êµ¬ë¶„ ë¬¸ìë¡œ ë¶„ì ˆí•˜ëŠ” SimpleTokenizerë¥¼ ìƒì†ë°›ì•„ ë‹¤ìŒì˜ tokenizerë“¤ì„ êµ¬í˜„í•˜ì‹œì˜¤.
#
# - uni-gram tokenizer
#   - ì˜ˆ : `ì‹ ì¸ ìƒ¹ì†¡ ê°€ìˆ˜ì˜ ì‹ ì¶˜ ìƒ¹ì†¡ ì‡¼` -> [`ì‹ ì¸`, `ìƒ¹ì†¡`, `ê°€ìˆ˜ì˜`, `ì‹ ì¶˜`, `ìƒ¹ì†¡`, `ì‡¼`]
# - bi-gram tokenizer with stride 1
#   - ì˜ˆ : `ì‹ ì¸ ìƒ¹ì†¡ ê°€ìˆ˜ì˜ ì‹ ì¶˜ ìƒ¹ì†¡ ì‡¼` -> [`ì‹ ì¸ ìƒ¹ì†¡`, `ìƒ¹ì†¡ ê°€ìˆ˜ì˜`, `ê°€ìˆ˜ì˜ ì‹ ì¶˜`, `ì‹ ì¶˜ ìƒ¹ì†¡`, `ìƒ¹ì†¡ ì‡¼`]
# - tri-gram tokenizer with stride 2
#   - ì˜ˆ : `ì‹ ì¸ ìƒ¹ì†¡ ê°€ìˆ˜ì˜ ì‹ ì¶˜ ìƒ¹ì†¡ ì‡¼` -> [`ì‹ ì¸ ìƒ¹ì†¡ ê°€ìˆ˜ì˜`, `ê°€ìˆ˜ì˜ ì‹ ì¶˜ ìƒ¹ì†¡`, `ìƒ¹ì†¡ ì‡¼`]

# %%
test = 'ì‹ ì¸ ìƒ¹ì†¡ ê°€ìˆ˜ì˜ ì‹ ì¶˜ ìƒ¹ì†¡ ì‡¼'

test = test.split(' ')
count = len(test)

test
[" ".join(test[i:i+2]) for i in range(0, count-1,1)]


# %%
from cote_vol_1.q3 import Q3
from typing import List

class SimpleTokenizer:
  def __init__(self, delimiter=' '):
    self.delimiter = delimiter
  def tokenize(self, sentence: str) -> List[str]:
    raise NotImplementedError
  def detokenize(self, tokens: List[str]) -> str:
    raise NotImplementedError


# %%
q3_dataset = Q3.load_dataset()


# %%
import itertools

class Tokenizer(SimpleTokenizer):
    def __init__(self, n) -> None:
        super().__init__()
        self.n = n
        if n == 1:
            self.stride = 0
        elif n == 2:
            self.stride = 1
        else:
            self.stride = 2

    def tokenize(self, sentence: str) -> List[str]:
        sentence = sentence.split(self.delimiter)
        count = len(sentence)
        if self.stride == 0:
            return sentence
        elif self.stride == 1:
            if count == 1:
                return sentence
            else:
                return [" ".join(sentence[i:i+2]) for i in range(0, count-1,1)]
        else:
            return [" ".join(sentence[i:i+self.stride+1]) for i in range(0, count,self.stride)]

    def detokenize(self, tokens: List[str]) -> str:
        result = []
        for token in tokens:
            if len(token.split(self.delimiter)) == 1:
                result.append([token])
                continue
            result.append(token.split(self.delimiter)[:-1])

        try:
            result.append(token.split(self.delimiter)[1:])
        except:
            print(tokens)

        return " ".join(chain.from_iterable(result))



# %%
test = 'ì‹ ì¸'
tk = Tokenizer(2)
(tk.tokenize(test))


# %%
unigram_tokenizer = Tokenizer(1)
bigram_tokenizer = Tokenizer(2)
trigram_tokenizer = Tokenizer(3)
get_ipython().run_line_magic('time', 'Q3.validate(unigram_tokenizer)')
get_ipython().run_line_magic('time', 'Q3.validate(bigram_tokenizer)')
get_ipython().run_line_magic('time', 'Q3.validate(trigram_tokenizer)')

# %% [markdown]
# # ì„¸ì…˜ 2 : ë„˜íŒŒì´ ì •ê¸€
#
# ![](https://creativemazes.com.au/wp-content/uploads/2019/04/CreativeMazes_Mandurah_HighRes-6.jpg)
# %% [markdown]
# ### Q4 : Numpyë¥¼ ì´ìš©í•˜ì—¬ linear regressionì„ êµ¬í˜„í•˜ì‹œì˜¤.
#
# - input : $x_{i} \in \mathbb{R}$
# - output : $y_{i} \in \mathbb{R}$
# - error function : MSE
# - learning algorithm : gradient descent

# %%
import numpy as np
import matplotlib.pyplot as plt

X = np.array([1,2,4,3,5])
Y = np.array([1,3,3,2,5])

print("X:", X)
print("Y:", Y)

plt.scatter(X, Y)


# %%
class LinearModel:
  def __init__(self):
    self.W = 1.
    self.b = 0.
    self.lr = 0.001
  def forward(self, X: np.array) -> (np.array, np.array):
    self.X = X
    y_hat = self.W*self.X + self.b
    return y_hat, np.square(np.subtract(y_hat, Y)).mean()

  def backward(self, loss: np.array):
    self.W = self.W - loss * self.lr
    self.b = self.b - loss * self.lr



epochs = 10000
model = LinearModel()
for _ in range(epochs):
  y_hat, loss = model.forward(X)
  model.backward(loss)
print(y_hat)

# %% [markdown]
# ### Q5 : Numpyë¥¼ ì´ìš©í•˜ì—¬ Convolution 2d ì—°ì‚°ì„ êµ¬í˜„í•˜ì‹œì˜¤.
#
# - kernel size : 3
# - stride : 1
# - padding : 1
# - biasëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
#
# ![](https://cdn-images-1.medium.com/max/1200/1*1okwhewf5KCtIPaFib4XaA.gif)

# %%
from cote_vol_1.q5 import Q5

x, w = Q5.load_dataset()


# %%
y = # conv2d ì—°ì‚° ê²°ê³¼ë¥¼ ììœ ë¡­ê²Œ ì‘ì„±í•˜ì„¸ìš”
Q5.validate(y)

# %% [markdown]
# # ì„¸ì…˜3 : í€´ì¦ˆ íƒ€ì„
#
# ![](https://blog.hubspot.com/hubfs/google-quiz.jpg)
# %% [markdown]
# ### Q6 : ì£¼ì–´ì§„ ë¦¬ìŠ¤íŠ¸ì˜ ë‹¨ì–´ ê°ê°ì— `ì€`/`ëŠ”` ì¡°ì‚¬ë¥¼ ì ì ˆíˆ ë¶™ì—¬ì£¼ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì‹œì˜¤.
#
# - Tip : ì¢…ì„± ìœ ë¬´ì— ë”°ë¼ ì ì ˆí•œ ì¡°ì‚¬ê°€ ì •í•´ì§‘ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¢…ì„±ì´ ìˆìœ¼ë©´ `ì€` ì¢…ì„±ì´ ì—†ìœ¼ë©´ `ëŠ”`ì´ ë¶™ì–´ì•¼ í•©ë‹ˆë‹¤. `ì• í”Œì€` | `ì‚¬ê³¼ëŠ”`

# %%
from cote_vol_1.q6 import Q6

dataset = Q6.load_dataset()


# %%
josa_append_dataset = # ììœ ë¡­ê²Œ ì¡°ì‚¬ê°€ ë¶™ì€ ë°ì´í„°ì…‹ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”!
Q6.validate(josa_append_dataset)

# %% [markdown]
# ### Q7 : í† í°í™”ëœ ë¬¸ì¥ì˜ ì¼ë¶€ë¥¼ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬í•˜ì—¬ ì–¸ì–´ ëª¨í˜•ì„ í•™ìŠµí•˜ê³ ì í•©ë‹ˆë‹¤. í† í°í™”ëœ ë¦¬ìŠ¤íŠ¸ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ë‹¤ìŒì˜ ê·œì¹™ì„ ì¤€ìˆ˜í•˜ëŠ” ë§ˆìŠ¤í‚¹ ëœ ë°ì´í„° ì§‘í•©ì„ ë§Œë“¤ì–´ ë³´ì„¸ìš”!
#
# - ë§ˆìŠ¤í‚¹ í† í°ì€ `<mask>` ìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.
# - ë§ˆìŠ¤í‚¹ í† í°ì˜ idëŠ” 6ë²ˆ ì…ë‹ˆë‹¤.
#
# > The training data generator chooses 15% of the token positions at random for
# prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time.

# %%
get_ipython().system('pip install datasets')
get_ipython().system('pip install transformers')


# %%
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("skt/kogpt2-base-v2")


# %%
from datasets import load_dataset
dataset = load_dataset('kor_ner')
texts = [record['text'] for record in dataset['train']]


# %%
# í† í°í™” ë¦¬ìŠ¤íŠ¸
tokens = [tokenizer.tokenize(text) for text in texts]
# í† í° IDí™” ë¦¬ìŠ¤íŠ¸
token_ids = [tokenizer.encode(text) for text in texts]


# %%
print("í† í°:", tokens[0][:10],"...")
print("ID:", token_ids[0][:10],"...")


# %%
# ììœ ë¡­ê²Œ ë§ˆìŠ¤í‚¹ëœ í† í° ë¦¬ìŠ¤íŠ¸ (í˜¹ì€ í† í° ID ë¦¬ìŠ¤íŠ¸) ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”!

# %% [markdown]
# # ë§ˆë¬´ë¦¬
#
# ![](https://item.kakaocdn.net/do/0936dbb0aaa6270bd577ad286a1525aa8b566dca82634c93f811198148a26065)
#
# ê³ ìƒì´ ë§ìœ¼ì…¨ìŠµë‹ˆë‹¤!
#
# ë§ˆì§€ë§‰ìœ¼ë¡œ, colab ì—ì„œ ì €ì¥ í›„ `íŒŒì¼` -> `ë‹¤ìš´ë¡œë“œ` í•˜ì—¬ ipynb íŒŒì¼ì„ ì¹´í†¡ë°©ì´ë‚˜, taeminlee@korea.ac.kr ìœ¼ë¡œ ì „ì†¡í•´ì£¼ì„¸ìš” ğŸ˜Š
#
# ê·¸ëŸ¼ ë‹¤ìŒì— ë˜ ë´ìš”~~

